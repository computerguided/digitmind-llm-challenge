![](/img/results_header.png)

## Introduction

For the game Digitmind, the absolute maximum number of optimal guesses that are theoretically required to break any code is 9.

The following shows the histogram of the number of optimal guesses for 100000 games with random codes.

![](/img/max_optimal_guesses.png)

There is a small chance that the model will need 9 guesses to break a code as can be seen in the table below.

| Guesses | Frequency | [%] |
| ---: | ---: | ---: |
| 1 | 0 | 0.000% |
| 2 | 0 | 0.000% |
| 3 | 54 | 0.054% |
| 4 | 2581 | 2.581% |
| 5 | 21018 | 21.018% |
| 6 | 45660 | 45.660% |
| 7 | 27032 | 27.032% |
| 8 | 3598 | 3.598% |
| 9 | 57 | 0.057% |

This is generated by the script [optimal_guesses.py](/optimal_guesses.py). In this script, the guesses are generated by taking a random code from all possible combinations that are left after processing the score, while not allowing to code to be guessed, unless it is the last possible combination left.

The estimation of the average number of guesses \( G \) follows from the equation:

$$
G = \sum_{i=1}^{9} g_i \times f_i = 6.08
$$

where \( g_i \) is the number of guesses and \( f_i \) is the frequency of the number of guesses.


Another interesting metric is the maximum number of combinations left at the beginning of a game. This is shown in the table below.

| Guess nr. | Combinations |
| ---: | ---: |
| 1 | 5040 |
| 2 | 1440 |
| 3 | 378 |
| 4 | 216 |
| 5 | 50 |
| 6 | 18 |
| 7 | 7 |
| 8 | 2 |
| 9 | 1 |

This follows a logarithmic curve as shown in the plot below.
![](/img/combinations_left.png)

## Experiment description
This document contains the results of the Digitmind Challenge.

The complete spreadsheet with all the results can be found [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vS9cA0PTOcrUxEoXLn8tLnVTAOQo_rp2iUwqm4y6cVsyvZmt4wiEfnFBKVDizbMHmyIZOVqz9jXheUP/pubhtml#).

## Parameters

The following parameters were used to evaluate the models:

- **Won**: The number of games won.
- **Guesses**: The average number of guesses per game.
- **Reasoning time per guess**: The average time taken to reason about each guess.
- **Reasoning time per game**: The average time taken to reason about all guesses in a game.
- **Certainty errors per game**: The percentage of games where the model made a mistake in the certainty of its guess.
- **Reasoning tokens per game**: The average number of tokens used to reason about the game.
- **Optimality**: The percentage of guesses that were optimal.





## Results



The following table shows the results of the two models. Clearly, the o3-mini model is better on all parameters - except for the optimality - and won the challenge (4 times the game was a tie).

| Parameter | o1 | o3-mini |
| :--- | :---: | :---: |
| Won | 6 | **10** |
| Guesses | 6.45 | **6.15** |
| Reasoning time per guess | 52 | **24** |
| Reasoning time per game | 336 | **153** |
| Certainty errors per game | 75% | **65%** |
| Reasoning tokens per game | 45491 | **34982** |
| Optimality | **74%** | 71% |

The histogram below shows the number of guesses per game for the two models.

![](/img/guesses_histogram.png)


## Prompt Engineering

Considering that the o3-mini model is the best model, it is interesting to investigate if performance might be improved when the prompt is changed.

The area where the reasoning of the model could be improved is obviously that it should not make suboptimal guesses. In the first run, the o3-mini model 

In the first version of the prompt, under the ["Strategy"](prompt.md#strategy) section it was only mentioned that:

>- Be sure to make _optimal_ guesses to minimize the number of guesses!
>- A guess is considered _optimal_ when all previous guesses would give the same score if the guess would be the actual code.

While this technically specifies the strategy, apparently it is not clear - or elaborate enough - for the model to make only optimal guesses.

Therefore, the file [prompt_v2.md](/prompt_v2.md) contains an improved version of the original prompt. Under the ["Strategy"](prompt_v2.md#strategy) section, a more elaborate description of the strategy is given, explaining in elaborate detail how the model can determine whether a guess is optimal or not.

All the original codes are used again (as stored in the [code.dat](/codes.dat) file) to be able to make a fair comparison.

The results of the previous version of the prompt in comparison with the results of the improved prompt in the following table:

| Parameter | o3-mini | o3-mini v2 |
| :--- | :---: | :---: |
| Guesses | 6.15 | **5.55** |
| Won | 3 | **12** |
| Reasoning time per guess | 52 | **22** |
| Reasoning time per game | 336 | **124** |
| Certainty errors per game | 75% | **40%** |
| Reasoning tokens per game | 45491 | **25094** |
| Optimality | 74% | **100%** (!) |

From the table it is clear that changing the prompt dramatically increased the performance across all parameters!

The performance of the o3-mini model with the new prompt is now on par with the performance that can be expected with the algorithms as implemented in a computer code breaker as described in the respository [Digitmind](https://github.com/computerguided/digitmind-python).

This clearly shows the power of the prompt engineering.

## Conclusion

On all parameters - except for the optimality - the o3-mini model seems to be better than the o1 model: it is faster, cheaper (less reasoning tokens), and makes fewer mistakes.

The o3-model won 4 times more games than the o1-model. The 4 games the o1 model lost, were all games in which the o1 model took 8 or 9 guesses. 

If these codes are tried again, there is a chance that the o1 model will take less guesses, as shown in the table below.

| Code | First try | Second try |
| :--- | :---: | :---: |
| 7960 | 9 | 5 |
| 3796 | 8 | 8 |
| 8249 | 8 | 7 |
| 2953 | 8 | 6 |

But this is only if the o1 model doesn't make any mistakes, e.g. 'wander off course'.

The following table shows the guesses of the o1 model when it tried to guess the code 7960 but failed to follow the optimal strategy. In the table the possible combinations are given from which the model should choose the next guess if it would have followed the optimal strategy. If the model chose a combination which was not part of this set, it is marked as not being an optimal guess.

As can be seen, at the 6th guess, there is only one possible combination left, but the model chose 7450 instead of 7960.

| nr. | Guess | Combinations | Optimal guess | Correct position | Wrong position |
| ---: | ---: | ---: | ---: | ---: | ---: |
| 1 | 0123 | 5040 | yes | 0 | 1 |
| 2 | 4567 | 1440 | no | 1 | 1 |
| 3 | 4609 | 288 | yes | 0 | 3 |
| 4 | 8640 | 17 | no | 1 | 1 |
| 5 | 9047 | 5 | yes | 0 | 3 |
| 6 | 7450 | 1 | no | 2 | 0 |
| 7 | 6539 | 1 | no | 0 | 2 |
| 8 | 2813 | 1 | no | 0 | 0 |
| 9 | 7960 | 1 | yes | 4 | 0 |



